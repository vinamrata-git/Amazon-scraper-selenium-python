{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba11ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary library importation \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-armenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from nltk.corpus import stopwords\n",
    "nlp = en_core_web_sm.load()\n",
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data \n",
    "df = pd.read_csv(\"Data/data_science_book.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-findings",
   "metadata": {},
   "source": [
    "# Data cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94135dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning and converting the string into integer and float of these columns for later use \n",
    "df[\"Rating stars\"] = df[\"Rating stars\"].apply(lambda x: re.findall(\"\\d+\\.\\d+\", str(x)))\n",
    "df[\"Rating stars\"] = df[\"Rating stars\"].apply(lambda x: 0 if len(x)==0 else float(x[0]))\n",
    "df[\"Rating count\"] = df[\"Rating count\"].map(lambda x: str(x).replace(\",\", \"\"))\n",
    "df[\"Rating count\"] = df[\"Rating count\"].apply(lambda x: 0 if len(x)==0 else int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4fabcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-january",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the Unnamed: 0 column and rename the misspelled column\n",
    "df = df.drop(\"Unnamed: 0\", axis = 1)\n",
    "df = df.rename(columns = {\"Nme of book\" :\"Name of book\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e271589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6804f5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-living",
   "metadata": {},
   "source": [
    "# Top 20 Highly rated book on Amazon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a0ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.nlargest(20, [\"Rating count\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df1[\"Name of book\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-restoration",
   "metadata": {},
   "source": [
    "In the most rated list of books we can see there are few books not related to data science because when I typed the keywords to search the books, I was not very specific I wrote \"Data science book\" but we can write very specific keywords to seach the products or anything and then scrape. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-dress",
   "metadata": {},
   "source": [
    "# 20 Highly rated and highest star Books on amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3593621",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.nlargest(20, [\"Rating stars\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df2[\"Name of book\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e727865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the scraped_text\n",
    "def text_cleaning (input_text):    \n",
    "    processed_text = str(input_text).lower()\n",
    "    processed_text = re.sub('[^a-zA-Z]', ' ', processed_text )\n",
    "    processed_text = re.sub(r'\\s+', ' ', processed_text)\n",
    "    return processed_text\n",
    "#input the text \n",
    "# processed_text = text_cleaning (doc)\n",
    "\n",
    "# Preparing the text\n",
    "def prepare_text(processed_text):\n",
    "    sentences = nltk.sent_tokenize(processed_text)\n",
    "    words = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    return words\n",
    "\n",
    "# Removing Stop Words\n",
    "def remove_stopwords(words):\n",
    "    lst_word = []                   #store all the cleaned dont contain stop words in this variable \n",
    "    for i in range(len(words)):\n",
    "        words[i] = [w for w in words[i] if w not in stopwords.words('english')]\n",
    "        lst_word.append(words[i])\n",
    "    return lst_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning, processing and removing stopwords from the name of book\n",
    "processed_text = [text_cleaning(i) for i in (df[\"Name of book\"])]\n",
    "words = [nltk.word_tokenize(sent) for sent in processed_text]\n",
    "filterd= remove_stopwords(words)\n",
    "def flatten(lst):\n",
    "    return [item for sublist in lst for item in sublist]\n",
    "flattened_lst = flatten(filterd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56763aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate wordscloud from name of books \n",
    "listToStr = ' '.join(map(str, flattened_lst))\n",
    "wordcloud = WordCloud(width=1600, height=800,background_color=\"white\").generate(listToStr)\n",
    "plt.figure( figsize=(20,10), facecolor='k')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ec9de5",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126a5950",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a way of measuring how relevant a word is to a document in a collection of documents.\n",
    "\n",
    "1. Term Frequency (TF): how many times a word appears in a document.\n",
    "2. Inverse Document Frequency (IDF): the inverse document frequency of the word across a collection of documents. Rare words have high scores, common words have low scores.\n",
    "\n",
    "* Use case of TFIDF:TF-IDF has many uses, such as in information retrieval, text analysis, keyword extraction, and as a way of obtaining numeric features from text for machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-greece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"Img/tfidf.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-health",
   "metadata": {},
   "source": [
    "# One word level Term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate the term frequency in the name of the book \n",
    "def term_frequency_calculator(lst_words):\n",
    "    #create an empty dictionary\n",
    "    data = {}\n",
    "    for item in lst_words:\n",
    "        #iterate the count of words over the dictionary\n",
    "        if item in data:\n",
    "            data[item]+=1\n",
    "        else:\n",
    "            data[item]=1\n",
    "    word_size = sum(v for k,v in data.items())\n",
    "    return [(word,freq/word_size) for word,freq in data.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of tuples \n",
    "lst_tuples = term_frequency_calculator(flattened_lst)\n",
    "#Create a dataframe from the list of tuples \n",
    "tf = pd.DataFrame(lst_tuples, columns = [\"Words\", \"Frequency\"])\n",
    "#tf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize =(12,8))\n",
    "tf_frequnt = tf.sort_values(by=['Frequency'], ascending=False)\n",
    "tf_frequnt = tf_frequnt.head(20)\n",
    "sns.barplot(x=\"Frequency\", y=\"Words\", data=tf_frequnt, color=\"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-ghana",
   "metadata": {},
   "source": [
    "# Ngram level TFIDF (Tri-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-joshua",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [' '.join(map(str,i)) for i in filterd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ab2706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ngrams \n",
    "vectorizer = CountVectorizer(ngram_range = (3,3))\n",
    "X1 = vectorizer.fit_transform(text) \n",
    "features = (vectorizer.get_feature_names())\n",
    "#print(\"\\n\\nFeatures : \\n\", features)\n",
    "#print(\"\\n\\nX1 : \\n\", X1.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-creek",
   "metadata": {},
   "source": [
    "# Calculate Term frequency using tfidfvecoriser library from sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3d803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tfidf vectorizer\n",
    "vect = TfidfVectorizer(ngram_range = (3,3))\n",
    "#fit the text to the model\n",
    "X = vect.fit_transform(text)\n",
    "#create columns as term and their frequency in different column \n",
    "features_rank = list(zip(vect.get_feature_names(), [round(x[0],1) for x in X.sum(axis=0).T.tolist()]))\n",
    "#create a dataframe \n",
    "df = pd.DataFrame(features_rank, columns =['Words', 'Frequency'])\n",
    "#Sort the values by frequency to plot the most frequent words \n",
    "tf = df.sort_values(by=['Frequency'], ascending=True)\n",
    "tf = tf.tail(20)\n",
    "fig = plt.figure(figsize =(12,8))\n",
    "sns.barplot(x=\"Frequency\", y=\"Words\", data=tf,\n",
    "            label=\"Total\", color=\"R\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-ghost",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
